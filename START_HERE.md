# ğŸ“ Classroom Engagement Analysis System
## START HERE - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

---

## ğŸ“‹ ì‹œìŠ¤í…œ êµ¬ì„±

ì´ í”„ë¡œì íŠ¸ëŠ” **í•™ê¸‰ ì°¸ì—¬ë„ ë¶„ì„**ì„ ìœ„í•œ í•™ìˆ  ìˆ˜ì¤€ì˜ AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```
ì…ë ¥ (Video/Audio)
    â†“
ê°ì§€ (YOLOv7) â†’ í•™ìƒ ì¸ì‹
    â†“
ì¶”ì  (DeepSORT) â†’ í•™ìƒ ID ìœ ì§€
    â†“
ìì„¸ ì¶”ì • (MediaPipe) â†’ ë¨¸ë¦¬/ì†/ëª¸ ìœ„ì¹˜
    â†“
íŠ¹ì§• ì¶”ì¶œ â†’ ì‹œì„ /ìì„¸/ì†/ìƒí˜¸ì‘ìš© ì‹ í˜¸
    â†“
ë¶„ë¥˜ (Rule-based) â†’ 0-100 ì°¸ì—¬ë„ ì ìˆ˜
    â†“
ë¦¬í¬íŠ¸ ìƒì„± (Gemini API) â†’ ì„¤ëª… ê°€ëŠ¥í•œ ê²°ê³¼
```

---

## âš¡ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

### Step 1: ì„¤ì¹˜
```bash
cd ~/Desktop/videoanalysis
pip install -r requirements.txt
```

### Step 2: ë¹„ë””ì˜¤ ì¤€ë¹„
```bash
mkdir -p data/raw outputs/reports outputs/visualizations
cp /path/to/video_example.mp4 data/raw/
```

### Step 3: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (30í”„ë ˆì„)
```bash
python -c "
import yaml
import cv2
from src import PersonDetector, EngagementClassifier

with open('config/config.yaml') as f:
    config = yaml.safe_load(f)

detector = PersonDetector('models/yolov7.pt', device='cpu', half_precision=False)
classifier = EngagementClassifier(config)

cap = cv2.VideoCapture('data/raw/video_example.mp4')
for i in range(30):
    ret, frame = cap.read()
    if ret:
        detections = detector.detect(frame)
        print(f'Frame {i}: {len(detections)} persons detected')

print('âœ“ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!')
"
```

### Step 4: ì „ì²´ ë¶„ì„
```bash
# ì œê³µëœ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python analyze_video_full.py

# ê²°ê³¼ëŠ” outputs/reports/ ì— ì €ì¥ë¨
ls -lh outputs/reports/
```

---

## ğŸ“Š ì£¼ìš” ì¶œë ¥ë¬¼

ë¶„ì„ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

| íŒŒì¼ | í˜•ì‹ | ìš©ë„ |
|------|------|------|
| `engagement_report_*.txt` | í…ìŠ¤íŠ¸ | ì½˜ì†”/ë©”ëª¨ |
| `engagement_report_*.html` | ì›¹ í˜ì´ì§€ | êµì‚¬ ë³´ê³ ì„œ (ì‹œê°ì ) |
| `engagement_data_*.json` | ë°ì´í„° | ì¶”ê°€ ë¶„ì„ìš© êµ¬ì¡°í™” ë°ì´í„° |
| `engagement_analysis.png` | ê·¸ë˜í”„ | ì‹œê°í™” ìš”ì•½ |

---

## ğŸ¯ í•µì‹¬ ê°œë…

### ì°¸ì—¬ë„ ì ìˆ˜ (Engagement Score)
```
0-33:   Low (âŒ ë‚®ì€ ì°¸ì—¬ë„ - ì£¼ì˜ í•„ìš”)
34-66:  Moderate (âš ï¸ ë³´í†µ - ê°œì„  ê¶Œê³ )
67-100: High (âœ“ ë†’ì€ ì°¸ì—¬ë„ - ì¢‹ìŒ)
```

### 4ê°€ì§€ ì¸¡ì • ì‹ í˜¸

| ì‹ í˜¸ | ì„¤ëª… | ê°€ì¤‘ì¹˜ |
|------|------|--------|
| ì‹œì„  (Gaze) | í™”ë©´ ë°©í–¥ ê°ë„ | 40% |
| ìì„¸ (Posture) | ì–´ê¹¨-ì—‰ë©ì´ ê¸°ìš¸ê¸° ì•ˆì •ì„± | 20% |
| ì† (Gesture) | ì†ì§“/í•„ê¸° ë¹ˆë„ | 20% |
| ìƒí˜¸ì‘ìš© (Interaction) | íƒœë¸”ë¦¿ ê·¼ì²˜ ê±°ë¦¬ | 20% |

ëª¨ë“  ì ìˆ˜ëŠ” **ì™„ì „íˆ í•´ì„ ê°€ëŠ¥**í•˜ë©° íŠ¹ì • ìˆ˜ì¹˜ì— ê¸°ë°˜í•©ë‹ˆë‹¤.

---

## ğŸ”§ ì£¼ìš” íŒŒì¼ ë° ì—­í• 

```
videoanalysis/
â”œâ”€â”€ config/config.yaml              â† ëª¨ë“  ì„¤ì • (ì„ê³„ê°’, ê°€ì¤‘ì¹˜, ëª¨ë¸)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ detector.py                 â† YOLOv7 (ì‚¬ëŒ ê°ì§€)
â”‚   â”œâ”€â”€ tracker.py                  â† DeepSORT (ID ìœ ì§€)
â”‚   â”œâ”€â”€ pose_estimator.py           â† MediaPipe (ìì„¸ ì¶”ì •)
â”‚   â”œâ”€â”€ feature_engineering.py      â† ì‹ í˜¸ ê³„ì‚°
â”‚   â”œâ”€â”€ engagement_classifier.py    â† ì ìˆ˜í™”
â”‚   â””â”€â”€ report_generator.py         â† Gemini ë¦¬í¬íŠ¸
â”œâ”€â”€ README.md                        â† ê¸°ìˆ  ìƒì„¸ ë¬¸ì„œ
â”œâ”€â”€ IMPLEMENTATION_GUIDE.md          â† êµ¬í˜„ ê°€ì´ë“œ
â”œâ”€â”€ TEST_GUIDE_VIDEO.md             â† ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ
â””â”€â”€ START_HERE.md                   â† ì´ ë¬¸ì„œ
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥

| êµ¬ì„± | ì²˜ë¦¬ ì†ë„ | GPU | ì •í™•ë„ |
|------|---------|-----|--------|
| **CPU (ê¸°ë³¸)** | ~5 FPS | ì—†ìŒ | ì¢‹ìŒ |
| **GPU (ê¶Œì¥)** | ~19 FPS | CUDA | ìš°ìˆ˜ |
| **ê²½ëŸ‰ ì„¤ì •** | ~30 FPS | CUDA | ë³´í†µ |

---

## â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q1: ì™œ ì‚¬ëŒì„ ê°ì§€í•˜ì§€ ëª»í• ê¹Œ?
**A**: ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
- ë¹„ë””ì˜¤ í•´ìƒë„ (ìµœì†Œ 480p)
- ì¡°ëª… ìƒíƒœ (ì¶©ë¶„í•œ ë°ê¸°)
- config.yamlì˜ `confidence_threshold` ë‚®ì¶”ê¸° (0.40ìœ¼ë¡œ)

### Q2: ì°¸ì—¬ë„ ì ìˆ˜ê°€ í•­ìƒ ë‚®ìŒ
**A**: ì„¤ì • ì¡°ì •:
```yaml
# config/config.yaml
features:
  gaze:
    eye_contact_threshold_deg: 40  # ê¸°ë³¸ 30ë„ì—ì„œ ì˜¬ë¦¬ê¸°
  posture:
    stability_variance_threshold_deg: 8  # ê¸°ë³¸ 5ë„ì—ì„œ ì˜¬ë¦¬ê¸°
```

### Q3: Gemini APIê°€ í•„ìš”í•œê°€?
**A**: ì•„ë‹ˆì˜¤. ì—†ì–´ë„ ê·œì¹™ ê¸°ë°˜ ì„¤ëª… ìë™ ìƒì„±. API ìˆìœ¼ë©´ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥.

### Q4: ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥í•œê°€?
**A**: GPUì—ì„œ ~19 FPS. êµì‹¤ìš© 30fps ì˜ìƒì€ ì•½ê°„ ì§€ì—° (0.1ì´ˆ).

### Q5: ê°œì¸ì •ë³´ëŠ” ì•ˆì „í•œê°€?
**A**: 
- âœ“ ì–¼êµ´ ì¸ì‹ ì•ˆ í•¨
- âœ“ ì‹ ì› ì¶”ì • ì•ˆ í•¨  
- âœ“ ê°ì • íŒë‹¨ ì•ˆ í•¨
- âœ“ ë°ì´í„°ëŠ” í–‰ë™ ì‹ í˜¸ë§Œ ì €ì¥

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ì…ë¬¸ì
1. âœ“ START_HERE.md ì½ê¸° (ì§€ê¸ˆ)
2. â†’ TEST_GUIDE_VIDEO.mdë¡œ ì²« ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸
3. â†’ ê²°ê³¼ ê²€í†  ë° config ì¡°ì •

### ì¤‘ê¸‰ ì‚¬ìš©ì
1. âœ“ IMPLEMENTATION_GUIDE.md ìˆ™ì§€
2. â†’ ì—¬ëŸ¬ ë¹„ë””ì˜¤ë¡œ ì‹œìŠ¤í…œ ê²€ì¦
3. â†’ í´ë˜ìŠ¤ë³„ ë§¤ê°œë³€ìˆ˜ ìµœì í™”
4. â†’ êµì‚¬ í”¼ë“œë°± ë°˜ì˜

### ê°œë°œì/ì—°êµ¬ì
1. âœ“ README.md ê¸°ìˆ  ì„¸ë¶€ì‚¬í•­ í™•ì¸
2. â†’ src/ ëª¨ë“ˆ ì»¤ìŠ¤í„°ë§ˆì´ì§•
3. â†’ ìƒˆë¡œìš´ íŠ¹ì§• ì¶”ê°€
4. â†’ ML ëª¨ë¸ í›ˆë ¨ (ì„ íƒì‚¬í•­)

---

## ğŸ“ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸

```
[ ] Python 3.8+ ì„¤ì¹˜?
    â†’ python --version

[ ] í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜?
    â†’ pip install -r requirements.txt

[ ] ë¹„ë””ì˜¤ íŒŒì¼ ì¡´ì¬?
    â†’ ls -lh data/raw/video_example.mp4

[ ] config.yaml í™•ì¸?
    â†’ cat config/config.yaml | head -20

[ ] ê²½ê³ /ì—ëŸ¬ ì—†ìŒ?
    â†’ python test_video_quick.py 2>&1 | grep -i error

[ ] ì‚¬ëŒ ê°ì§€ë¨?
    â†’ í•´ìƒë„ >= 480p, ì¡°ëª… ì¶©ë¶„í•œì§€ í™•ì¸

[ ] ì ìˆ˜ê°€ í•©ë¦¬ì ì¸ê°€?
    â†’ 0-100 ë²”ìœ„, 0-33 ë‚®ìŒ/34-66 ë³´í†µ/67-100 ë†’ìŒ
```

---

## ğŸ“– ìƒì„¸ ë¬¸ì„œ ë§µ

```
START_HERE.md (ì§€ê¸ˆ ì½ëŠ” ê³³)
    â†“
TEST_GUIDE_VIDEO.md â† ì‹¤ì œ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ
    â†“
IMPLEMENTATION_GUIDE.md â† ì™„ì „í•œ êµ¬í˜„ ë° ë°°í¬
    â†“
README.md â† ê¸°ìˆ  ê¹Šì´ ìˆëŠ” ì„¤ëª…
    â†“
src/*/code â† ì†ŒìŠ¤ ì½”ë“œ ë° ì¸ë¼ì¸ ë¬¸ì„œ
```

---

## ğŸ’¡ íŒ

### ë¹ ë¥¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```bash
# ì²« 100í”„ë ˆì„ë§Œ ì²˜ë¦¬ (ë””ë²„ê¹…ìš©)
python -c "
import cv2
from src import PersonDetector

detector = PersonDetector('models/yolov7.pt', device='cpu', half_precision=False)
cap = cv2.VideoCapture('data/raw/video_example.mp4')

for i in range(100):
    ret, frame = cap.read()
    if ret:
        detections = detector.detect(frame)
        print(f'Frame {i}: {len(detections)} detected')
"
```

### íŠ¹ì • ì‹œê°„ëŒ€ ë¶„ì„
```bash
# 10-20ì´ˆë§Œ ì²˜ë¦¬
python -c "
import cv2
fps = 30
cap = cv2.VideoCapture('data/raw/video_example.mp4')
start_frame = int(10 * fps)
end_frame = int(20 * fps)

cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
for i in range(start_frame, end_frame):
    ret, frame = cap.read()
    # ë¶„ì„...
"
```

---

## âœ… ì‘ë™ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹œìŠ¤í…œì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸:

```bash
# 1. íŒ¨í‚¤ì§€ ì„í¬íŠ¸
python -c "from src import PersonDetector, EngagementClassifier; print('âœ“ íŒ¨í‚¤ì§€ OK')"

# 2. config ë¡œë“œ
python -c "import yaml; yaml.safe_load(open('config/config.yaml')); print('âœ“ ì„¤ì • OK')"

# 3. ë¹„ë””ì˜¤ ì—´ê¸°
python -c "import cv2; cv2.VideoCapture('data/raw/video_example.mp4').isOpened() and print('âœ“ ë¹„ë””ì˜¤ OK')"

# 4. ëª¨ë“  ê²ƒ ì¢…í•©
python test_video_quick.py
```

ëª¨ë‘ âœ“ì´ë©´ ì¤€ë¹„ ì™„ë£Œ!

---

## ğŸ“ License & Citation

í•™ìˆ  ëª©ì  ì¸ìš©:
```bibtex
@software{classroom_engagement_2025,
  title={Classroom Engagement Analysis System},
  author={Your Name},
  year={2025},
  url={https://github.com/...}
}
```

---

**ì´ì œ TEST_GUIDE_VIDEO.mdë¡œ ì§„í–‰í•˜ì„¸ìš”! ğŸš€**
